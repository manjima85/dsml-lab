# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U4QCvydo5EZAJk8B4Mkq3M0oJCB7yD62
"""

import pandas as pd
import numpy as np
import nltk
from  nltk.tokenize import word_tokenize
from nltk import pos_tag

from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.preprocessing import LabelEncoder
from collections import defaultdict
from nltk.corpus import wordnet as wn
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import model_selection, naive_bayes, svm
from sklearn.metrics import accuracy_score

from sklearn.metrics import accuracy_score
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

np.random.seed(500)

Corpus = pd.read_csv("/content/corpus.csv",encoding='latin-1')

Corpus['text'].dropna(inplace=True)

Corpus['text'] = [entry.lower() for entry in Corpus['text']]

Corpus['text']=[word_tokenize(entry) for entry in Corpus['text']]

tag_map = defaultdict(lambda : wn.NOUN)
tag_map['J']=wn.ADJ
tag_map['V']=wn.VERB
tag_map['R']=wn.ADV
for index,entry in enumerate(Corpus['text']):
  Final_words=[]
  word_Lemmatized = WordNetLemmatizer()
  for word, tag in pos_tag(entry):
    if word not in stopwords.words('english') and word.isalpha():
      word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])
      Final_words.append(word_Final)
  Corpus.loc[index,'text_final'] = str(Final_words)

Train_X,Test_X,Train_Y,Test_Y=model_selection.train_test_split(Corpus['text_final'],Corpus['label'],test_size=0.3)

Encoder = LabelEncoder()
Train_Y = Encoder.fit_transform(Train_Y)
Test_Y = Encoder.fit_transform(Test_Y)
print(Train_Y)

Tfidf_vect=TfidfVectorizer (max_features=5000)
Tfidf_vect.fit(Corpus ['text_final'])
Train_X_Tfidf = Tfidf_vect.transform(Train_X)
Test_X_Tfidf = Tfidf_vect.transform(Test_X)
print(Tfidf_vect.vocabulary_)

print(Train_X_Tfidf)

SVM =svm.SVC(C=1.0, kernel="linear", degree=3, gamma= 'auto')
SVM.fit(Train_X_Tfidf, Train_Y)

predictions_SVM=SVM.predict(Test_X_Tfidf)

print("SVM Accuracy Score -> ", accuracy_score (predictions_SVM, Test_Y)*100)

new_text = "This is a new text that needs to be classified."
tokens = word_tokenize(new_text)
tokens = [token. lower() for token in tokens]
lemmatizer=WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize (token) for token in tokens]

stop_words = set (stopwords.words("english"))
filtered_tokens = [token for token in tokens if token not in stop_words]
preprocessed_text = ' '.join(tokens)
new_text_vector= Tfidf_vect.transform([preprocessed_text])
predicted_class = SVM.predict(new_text_vector)
print("Predicted class:", predicted_class)